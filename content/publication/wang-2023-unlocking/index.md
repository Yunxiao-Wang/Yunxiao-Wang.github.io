---
title: Unlocking the Power of Multimodal Learning for Emotion Recognition in Conversation
authors:
- Yunxiao_Wang
- Meng Liu
- Zhe Li
- Yupeng Hu
- Xin Luo
- Liqiang Nie
date: '2023-10-27'
publishDate: '2024-01-11T13:39:48.408545Z'
publication_types:
- paper-conference
publication: '*Proceedings of the 31st ACM International Conference on Multimedia*'
publication_short: "*ACM MM 2023*"

abstract: Emotion recognition in conversation aims to identify the emotions underlying each utterance, and it has great potential in various domains. Human perception of emotions relies on multiple modalities, such as language, vocal tonality, and facial expressions. While many studies have incorporated multimodal information to enhance emotion recognition, the performance of multimodal models often plateaus when additional modalities are added. We demonstrate through experiments that the main reason for this plateau is an imbalanced assignment of gradients across modalities. To address this issue, we propose fine-grained adaptive gradient modulation, a plug-in approach to rebalance the gradients of modalities. Experimental results show that our method improves the performance of all baseline models and outperforms existing plug-in methods.

url_pdf: https://dl.acm.org/doi/pdf/10.1145/3581783.3613846
url_code: https://acmmm2023.wixsite.com/fagm

image:
#   caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/s9CC2SKySJM)'
  focal_point: ""
  preview_only: false
---
