---
title: Siamese alignment network for weakly supervised video moment retrieval
authors:
- Yunxiao Wang
- Meng Liu
- Yinwei Wei
- Zhiyong Cheng
- Yinglong Wang
- Liqiang Nie
date: '2022-04-19'
publishDate: '2024-01-11T13:39:48.393329Z'
publication_types:
- article-journal
publication: '*IEEE Transactions on Multimedia*'
publication_short: '*TMM*'

abstract: Video moment retrieval, i.e., localizing the specific video moments within a video given a description query, has attracted substantial attention over the past several years. Although great progress has been achieved thus far, most of existing methods are supervised, which require moment-level temporal annotation information. In contrast, weakly-supervised methods which only need video-level annotations remain largely unexplored. In this paper, we propose a novel end-to-end Siamese alignment network for weakly-supervised video moment retrieval. To be specific, we design a multi-scale Siamese module, which could progressively reduce the semantic gap between the visual and textual modality with the Siamese structure. In addition, we present a context-aware multiple instance learning module by considering the influence of adjacent contexts, enhancing the moment-query and video-query alignment simultaneously. By promoting the matching of both moment-level and video-level, our model can effectively improve the retrieval performance, even if only having weak video level annotations. Extensive experiments on two benchmark datasets, i.e., ActivityNet-Captions and Charades-STA, verify the superiority of our model compared with several state-of-the-art baselines.

url_pdf: https://drive.google.com/file/d/1zDUAav9KPINS_3_5FHlUf-8RaX6kh29K/view
url_code: https://sancode.wixsite.com/san-model
image:
#   caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/s9CC2SKySJM)'
  focal_point: ""
  preview_only: false
---
